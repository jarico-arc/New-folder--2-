# Comprehensive Prometheus Stack for YugabyteDB Monitoring
# Security: No hardcoded passwords, secure defaults
# Includes SLO burn-rate rules and custom dashboards

apiVersion: v1
kind: Namespace
metadata:
  name: monitoring
  labels:
    name: monitoring
    pod-security.kubernetes.io/enforce: restricted
    pod-security.kubernetes.io/audit: restricted
    pod-security.kubernetes.io/warn: restricted

---
# Secret for Grafana admin password - MUST be created separately
apiVersion: v1
kind: Secret
metadata:
  name: grafana-admin-secret
  namespace: monitoring
type: Opaque
data:
  # SECURITY: Admin password must be generated and set externally
  # Generate using: make generate-grafana-secret
  # Or manually: kubectl create secret generic grafana-admin-secret -n monitoring \
  #   --from-literal=admin-user=admin \
  #   --from-literal=admin-password="$(openssl rand -base64 32)"
  admin-user: YWRtaW4=  # admin (base64)
  admin-password: ""  # MUST be set via external secret management - DO NOT COMMIT

---
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: kube-prometheus-stack
  namespace: argocd
  annotations:
    argocd.argoproj.io/sync-wave: "2"
spec:
  destination:
    namespace: monitoring
    server: https://kubernetes.default.svc
  project: default
  source:
    chart: kube-prometheus-stack
    repoURL: https://prometheus-community.github.io/helm-charts
    targetRevision: "57.2.0"
    helm:
      values: |
        # Global configuration with security hardening
        global:
          rbac:
            createAggregateClusterRoles: false
          imagePullSecrets: []
        
        # Prometheus configuration with enhanced security
        prometheus:
          prometheusSpec:
            retention: 30d
            retentionSize: "100GiB"
            # Security: Non-root container
            securityContext:
              runAsNonRoot: true
              runAsUser: 65534
              fsGroup: 65534
            # Resource management
            resources:
              requests:
                cpu: "2"
                memory: 8Gi
              limits:
                cpu: "4"
                memory: 16Gi
            # Persistent storage configuration
            storageSpec:
              volumeClaimTemplate:
                spec:
                  storageClassName: ssd-monitoring
                  accessModes: ["ReadWriteOnce"]
                  resources:
                    requests:
                      storage: 200Gi
            # Service discovery configuration
            serviceMonitorSelectorNilUsesHelmValues: false
            serviceMonitorSelector: {}
            podMonitorSelectorNilUsesHelmValues: false
            podMonitorSelector: {}
            ruleSelectorNilUsesHelmValues: false
            ruleSelector: {}
            
            # Enhanced scrape configs for YugabyteDB multi-cluster
            additionalScrapeConfigs:
              - job_name: 'yugabyte-master-helm'
                scrape_interval: 30s
                scrape_timeout: 10s
                metrics_path: /prometheus-metrics
                kubernetes_sd_configs:
                  - role: pod
                    namespaces:
                      names: ['codet-dev-yb', 'codet-staging-yb', 'codet-prod-yb']
                relabel_configs:
                  - source_labels: [__meta_kubernetes_pod_label_app]
                    action: keep
                    regex: yb-master
                  - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
                    action: keep
                    regex: true
                  - source_labels: [__meta_kubernetes_pod_ip]
                    action: replace
                    target_label: __address__
                    regex: (.+)
                    replacement: ${1}:7000
                  - source_labels: [__meta_kubernetes_namespace]
                    action: replace
                    target_label: kubernetes_namespace
                  - source_labels: [__meta_kubernetes_pod_name]
                    action: replace
                    target_label: kubernetes_pod_name
                    
              - job_name: 'yugabyte-tserver-helm'
                scrape_interval: 30s
                scrape_timeout: 10s
                metrics_path: /prometheus-metrics
                kubernetes_sd_configs:
                  - role: pod
                    namespaces:
                      names: ['codet-dev-yb', 'codet-staging-yb', 'codet-prod-yb']
                relabel_configs:
                  - source_labels: [__meta_kubernetes_pod_label_app]
                    action: keep
                    regex: yb-tserver
                  - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
                    action: keep
                    regex: true
                  - source_labels: [__meta_kubernetes_pod_ip]
                    action: replace
                    target_label: __address__
                    regex: (.+)
                    replacement: ${1}:9000
                  - source_labels: [__meta_kubernetes_namespace]
                    action: replace
                    target_label: kubernetes_namespace
                  - source_labels: [__meta_kubernetes_pod_name]
                    action: replace
                    target_label: kubernetes_pod_name
        
        # Grafana configuration with security improvements
        grafana:
          # Security: Use external secret for admin password
          admin:
            existingSecret: grafana-admin-secret
            userKey: admin-user
            passwordKey: admin-password
          
          # Security context
          securityContext:
            runAsNonRoot: true
            runAsUser: 472
            fsGroup: 472
          
          # Resource management
          resources:
            requests:
              cpu: 500m
              memory: 1Gi
            limits:
              cpu: "1"
              memory: 2Gi
          
          # Persistent storage
          persistence:
            enabled: true
            storageClassName: ssd-monitoring
            size: 10Gi
            accessModes:
              - ReadWriteOnce
          
          # Security: Disable anonymous access
          grafana.ini:
            security:
              disable_initial_admin_creation: false
              admin_user: admin
              cookie_secure: true
              cookie_samesite: strict
            auth.anonymous:
              enabled: false
            auth:
              disable_login_form: false
            server:
              protocol: http
              enforce_domain: false
              root_url: ""
          
          # Pre-configured dashboards for YugabyteDB
          dashboardProviders:
            dashboardproviders.yaml:
              apiVersion: 1
              providers:
              - name: 'yugabyte'
                orgId: 1
                folder: 'YugabyteDB'
                type: file
                disableDeletion: false
                editable: true
                updateIntervalSeconds: 30
                options:
                  path: /var/lib/grafana/dashboards/yugabyte
          
          dashboards:
            yugabyte:
              yugabyte-overview:
                gnetId: 12620
                revision: 1
                datasource: Prometheus
              yugabyte-cluster:
                gnetId: 12621  
                revision: 1
                datasource: Prometheus
              kubernetes-cluster:
                gnetId: 7249
                revision: 1
                datasource: Prometheus
          
          # Data sources with secure configuration
          additionalDataSources:
            - name: Prometheus
              type: prometheus
              url: http://kube-prometheus-stack-prometheus:9090
              access: proxy
              isDefault: true
              editable: false
        
        # AlertManager configuration with enhanced security
        alertmanager:
          alertmanagerSpec:
            # Security context
            securityContext:
              runAsNonRoot: true
              runAsUser: 65534
              fsGroup: 65534
            
            # Resource management
            resources:
              requests:
                cpu: 100m
                memory: 256Mi
              limits:
                cpu: 500m
                memory: 512Mi
            
            # Persistent storage
            storage:
              volumeClaimTemplate:
                spec:
                  storageClassName: ssd-monitoring
                  accessModes: ["ReadWriteOnce"]
                  resources:
                    requests:
                      storage: 10Gi
          
          # AlertManager configuration - externalize sensitive data
          config:
            global:
              # FIXED: Replace localhost with proper SMTP service
              smtp_smarthost: 'smtp-relay.kube-system.svc.cluster.local:587'
              smtp_from: 'alertmanager@codet.com'
              smtp_require_tls: true
            route:
              group_by: ['alertname', 'cluster', 'service', 'severity']
              group_wait: 10s
              group_interval: 10s
              repeat_interval: 1h
              receiver: 'default-receiver'
              routes:
              - match:
                  severity: critical
                receiver: 'pager-critical'
                group_wait: 0s
                repeat_interval: 5m
              - match:
                  severity: warning
                receiver: 'slack-warnings'
                repeat_interval: 12h
            receivers:
            - name: 'default-receiver'
              webhook_configs:
              # FIXED: Replace localhost with proper webhook service
              - url: 'http://webhook-service.monitoring.svc.cluster.local:5001/webhook'
                send_resolved: true
            - name: 'pager-critical'
              # Configure with your PagerDuty integration key
              webhook_configs:
              - url: 'https://events.pagerduty.com/v2/enqueue'
                send_resolved: true
                http_config:
                  bearer_token_file: /etc/alertmanager/secrets/pagerduty-key
            - name: 'slack-warnings'
              # Configure with your Slack webhook URL
              slack_configs:
              - api_url_file: /etc/alertmanager/secrets/slack-webhook-url
                channel: '#alerts'
                title: 'YugabyteDB Alert - {{ .GroupLabels.alertname }}'
                text: '{{ range .Alerts }}{{ .Annotations.summary }}{{ end }}'
                send_resolved: true

        # Node Exporter with security hardening
        nodeExporter:
          securityContext:
            runAsNonRoot: true
            runAsUser: 65534
        
        # Kube State Metrics with security hardening
        kubeStateMetrics:
          securityContext:
            runAsNonRoot: true
            runAsUser: 65534

---
# Custom ServiceMonitor for YugabyteDB with enhanced configuration
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: yugabyte-monitoring
  namespace: monitoring
  labels:
    app: yugabytedb
    release: kube-prometheus-stack
spec:
  selector:
    matchLabels:
      app: yugabytedb
  namespaceSelector:
    matchNames:
    # FIXED: Updated to current namespace structure
    - codet-dev-yb
    - codet-staging-yb  
    - codet-prod-yb
  endpoints:
  - port: http-ui
    path: /prometheus-metrics
    interval: 30s
    scrapeTimeout: 10s
  - port: http-ycql-met
    path: /prometheus-metrics
    interval: 30s
    scrapeTimeout: 10s

---
# PrometheusRule for YugabyteDB SLOs
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: yugabytedb-slos
  namespace: monitoring
  labels:
    app: yugabytedb
    release: kube-prometheus-stack
spec:
  groups:
  - name: yugabytedb.slos
    interval: 30s
    rules:
    - alert: YugabyteDBHighLatency
      expr: histogram_quantile(0.99, rate(rpc_latency_bucket[5m])) > 100
      for: 2m
      labels:
        severity: warning
      annotations:
        summary: "YugabyteDB high latency detected"
        description: "99th percentile latency is {{ $value }}ms"
    
    - alert: YugabyteDBNodeDown
      expr: up{job=~"yugabyte-.*"} == 0
      for: 1m
      labels:
        severity: critical
      annotations:
        summary: "YugabyteDB node is down"
        description: "Node {{ $labels.instance }} has been down for more than 1 minute"
    
    - alert: YugabyteDBReplicationLag
      expr: max(tablet_consensus_log_reader_bytes_behind) > 1048576
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "YugabyteDB replication lag detected"
        description: "Replication lag is {{ $value }} bytes"

---
# NetworkPolicy to secure monitoring namespace
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: monitoring-network-policy
  namespace: monitoring
spec:
  podSelector: {}
  policyTypes:
  - Ingress
  - Egress
  ingress:
  # Allow incoming connections from ingress controllers
  - from:
    - namespaceSelector:
        matchLabels:
          name: ingress-nginx
    ports:
    - protocol: TCP
      port: 3000  # Grafana
  # Allow internal monitoring communication
  - from:
    - podSelector: {}
    ports:
    - protocol: TCP
      port: 9090  # Prometheus
    - protocol: TCP
      port: 3000  # Grafana
    - protocol: TCP
      port: 9093  # AlertManager
  egress:
  # Allow outgoing connections to monitored services
  - to: []
    ports:
    - protocol: TCP
      port: 7000  # YugabyteDB Master
    - protocol: TCP
      port: 9000  # YugabyteDB TServer
    - protocol: TCP
      port: 443   # HTTPS
    - protocol: TCP
      port: 80    # HTTP
    - protocol: UDP
      port: 53    # DNS

---
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: yugabyte-tserver-monitoring
  namespace: monitoring
  labels:
    app: yugabytedb
    release: kube-prometheus-stack
spec:
  selector:
    matchLabels:
      app: yb-tserver
  namespaceSelector:
    matchNames:
    # FIXED: Updated to current namespace structure
    - codet-prod-yb
    - codet-staging-yb
    - codet-dev-yb  
  endpoints:
  - port: http-ysql-met
    interval: 15s
    path: /prometheus-metrics
  - port: http-ycql-met
    interval: 15s
    path: /prometheus-metrics

---
# Redpanda ServiceMonitor
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: redpanda-monitoring
  namespace: monitoring
  labels:
    app: redpanda
    release: kube-prometheus-stack
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: redpanda
  namespaceSelector:
    matchNames:
    - kafka
  endpoints:
  - port: prometheus
    interval: 15s
    path: /metrics 