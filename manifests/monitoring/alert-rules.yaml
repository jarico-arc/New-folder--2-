# Prometheus Alert Rules for YugabyteDB
# This file defines alerting rules for monitoring YugabyteDB health and performance

---
apiVersion: v1
kind: ConfigMap
metadata:
  name: yugabytedb-alert-rules
  namespace: monitoring
  labels:
    app: prometheus
data:
  yugabytedb.yml: |
    groups:
    - name: yugabytedb.rules
      rules:
      # Master Node Alerts
      - alert: YBMasterDown
        expr: up{job="yugabytedb-master"} == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "YugabyteDB Master node is down"
          description: "YugabyteDB Master node {{ $labels.instance }} in namespace {{ $labels.namespace }} has been down for more than 1 minute."

      - alert: YBMasterQuorumLost
        expr: sum(up{job="yugabytedb-master"}) by (namespace) < 2
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "YugabyteDB Master quorum lost"
          description: "YugabyteDB Master quorum lost in namespace {{ $labels.namespace }}. Only {{ $value }} masters are available."

      # TServer Node Alerts
      - alert: YBTServerDown
        expr: up{job="yugabytedb-tserver"} == 0
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "YugabyteDB TServer node is down"
          description: "YugabyteDB TServer node {{ $labels.instance }} in namespace {{ $labels.namespace }} has been down for more than 2 minutes."

      - alert: YBTServerCriticallyLow
        expr: sum(up{job="yugabytedb-tserver"}) by (namespace) < 2
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "YugabyteDB TServer count critically low"
          description: "Only {{ $value }} TServer nodes available in namespace {{ $labels.namespace }}. This may affect data availability."

      # Resource Usage Alerts
      - alert: YBHighCPUUsage
        expr: (rate(cpu_usage_user{job="yugabytedb-tserver"}[5m]) + rate(cpu_usage_system{job="yugabytedb-tserver"}[5m])) > 0.8
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "YugabyteDB high CPU usage"
          description: "YugabyteDB TServer {{ $labels.instance }} CPU usage is above 80% for more than 10 minutes."

      - alert: YBHighMemoryUsage
        expr: memory_usage{job="yugabytedb-tserver"} / memory_limit{job="yugabytedb-tserver"} > 0.9
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "YugabyteDB high memory usage"
          description: "YugabyteDB TServer {{ $labels.instance }} memory usage is above 90% for more than 5 minutes."

      # Connection Alerts
      - alert: YBHighConnectionCount
        expr: yb_node_connections{job="yugabytedb-tserver"} > 100
        for: 15m
        labels:
          severity: warning
        annotations:
          summary: "YugabyteDB high connection count"
          description: "YugabyteDB TServer {{ $labels.instance }} has more than 100 connections for 15 minutes."

      # Disk Usage Alerts
      - alert: YBHighDiskUsage
        expr: disk_usage_percent{job="yugabytedb-tserver"} > 85
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "YugabyteDB high disk usage"
          description: "YugabyteDB TServer {{ $labels.instance }} disk usage is above 85%."

      - alert: YBCriticalDiskUsage
        expr: disk_usage_percent{job="yugabytedb-tserver"} > 95
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "YugabyteDB critical disk usage"
          description: "YugabyteDB TServer {{ $labels.instance }} disk usage is above 95%. Immediate action required."

      # Replication Lag Alerts
      - alert: YBHighReplicationLag
        expr: replication_lag_ms{job="yugabytedb-tserver"} > 10000
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "YugabyteDB high replication lag"
          description: "YugabyteDB replication lag is higher than 10 seconds on {{ $labels.instance }}."

      # Cluster Health Alerts
      - alert: YBClusterUnhealthy
        expr: cluster_health_status{job="yugabytedb-master"} != 0
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "YugabyteDB cluster is unhealthy"
          description: "YugabyteDB cluster in namespace {{ $labels.namespace }} is reporting unhealthy status."

      # Network Partition Alerts
      - alert: YBNetworkPartition
        expr: increase(network_errors_total{job="yugabytedb-tserver"}[5m]) > 10
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "YugabyteDB network errors detected"
          description: "YugabyteDB TServer {{ $labels.instance }} is experiencing network errors. This may indicate network partition."

    # General Kubernetes Alerts for YugabyteDB
    - name: yugabytedb-kubernetes.rules
      rules:
      - alert: YBPodCrashLooping
        expr: rate(kube_pod_container_status_restarts_total{container=~"yb-.*"}[15m]) > 0
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "YugabyteDB pod is crash looping"
          description: "YugabyteDB pod {{ $labels.pod }} in namespace {{ $labels.namespace }} is crash looping."

      - alert: YBPodNotReady
        expr: kube_pod_status_ready{condition="false", pod=~".*yb.*"} == 1
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "YugabyteDB pod not ready"
          description: "YugabyteDB pod {{ $labels.pod }} in namespace {{ $labels.namespace }} has been not ready for more than 10 minutes."

      - alert: YBPersistentVolumeClaimPending
        expr: kube_persistentvolumeclaim_status_phase{phase="Pending", persistentvolumeclaim=~".*yb.*"} == 1
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "YugabyteDB PVC is pending"
          description: "PersistentVolumeClaim {{ $labels.persistentvolumeclaim }} for YugabyteDB in namespace {{ $labels.namespace }} is pending."

---
# AlertManager Configuration
apiVersion: v1
kind: ConfigMap
metadata:
  name: alertmanager-config
  namespace: monitoring
data:
  alertmanager.yml: |
    global:
      smtp_smarthost: 'smtp.gmail.com:587'  # Update with your SMTP server
      smtp_from: 'alertmanager@codet.com'
    
    route:
      group_by: ['alertname', 'cluster', 'service']
      group_wait: 10s
      group_interval: 10s
      repeat_interval: 1h
      receiver: 'default'
      routes:
      - match:
          severity: critical
        receiver: 'critical-alerts'
      - match:
          environment: production
        receiver: 'production-alerts'
    
    receivers:
    - name: 'default'
      webhook_configs:
      - url: 'http://slack-webhook-service.monitoring.svc.cluster.local:5001/webhook'
        send_resolved: true
    
    - name: 'critical-alerts'
      email_configs:
      - to: 'admin@codet.com'
        subject: 'CRITICAL: YugabyteDB Alert'
        body: |
          Alert: {{ .GroupLabels.alertname }}
          Description: {{ range .Alerts }}{{ .Annotations.description }}{{ end }}
          
    - name: 'production-alerts'
      email_configs:
      - to: 'oncall@codet.com'
        subject: 'PRODUCTION: YugabyteDB Alert'
        body: |
          PRODUCTION ALERT
          Alert: {{ .GroupLabels.alertname }}
          Description: {{ range .Alerts }}{{ .Annotations.description }}{{ end }}
      webhook_configs:
      - url: 'http://slack-webhook-service.monitoring.svc.cluster.local:5001/webhook'
        send_resolved: true

---
# AlertManager Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: alertmanager
  namespace: monitoring
spec:
  replicas: 1
  selector:
    matchLabels:
      app: alertmanager
  template:
    metadata:
      labels:
        app: alertmanager
    spec:
      containers:
      - name: alertmanager
        image: prom/alertmanager:latest
        ports:
        - containerPort: 9093
        volumeMounts:
        - name: config-volume
          mountPath: /etc/alertmanager
        args:
        - '--config.file=/etc/alertmanager/alertmanager.yml'
        - '--storage.path=/alertmanager'
        - '--web.external-url=http://alertmanager.monitoring.svc.cluster.local:9093'
      volumes:
      - name: config-volume
        configMap:
          name: alertmanager-config

---
# AlertManager Service
apiVersion: v1
kind: Service
metadata:
  name: alertmanager
  namespace: monitoring
spec:
  selector:
    app: alertmanager
  ports:
  - port: 9093
    targetPort: 9093
  type: ClusterIP 

# SLO Alert Rules for YugabyteDB - 99.95% Availability Target
# Implements burn-rate alerts (5 min + 1 hr) as per comprehensive plan

apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: yugabyte-slo-alerts
  namespace: monitoring
  labels:
    app: yugabytedb
    release: kube-prometheus-stack
spec:
  groups:
  - name: yugabyte.slo.rules
    interval: 15s
    rules:
    # SLI: Availability based on healthy masters/tservers
    - record: yugabyte:availability_sli
      expr: |
        min(
          min by (namespace) (up{job="yugabyte-master"}) * on (namespace) group_left
          min by (namespace) (up{job="yugabyte-tserver"})
        ) or vector(0)
      
    # SLI: Latency p95 < 150ms for YSQL
    - record: yugabyte:latency_sli  
      expr: |
        (
          histogram_quantile(0.95, 
            rate(yb_sql_query_response_time_bucket{job="yugabyte-tserver"}[5m])
          ) < 0.15
        ) or vector(0)
    
    # SLI: CDC lag < 60s
    - record: yugabyte:cdc_sli
      expr: |
        (
          max by (namespace) (yb_cdc_lag_seconds{job="yugabyte-tserver"}) < 60
        ) or vector(1)  # Default to 1 if no CDC metrics
    
    # Combined SLO (all conditions must be met)
    - record: yugabyte:slo_status
      expr: |
        yugabyte:availability_sli * yugabyte:latency_sli * yugabyte:cdc_sli
    
    # 5-minute burn rate (critical)
    - record: yugabyte:error_rate_5m
      expr: |
        1 - avg_over_time(yugabyte:slo_status[5m])
    
    # 1-hour burn rate (warning)  
    - record: yugabyte:error_rate_1h
      expr: |
        1 - avg_over_time(yugabyte:slo_status[1h])

  - name: yugabyte.slo.alerts
    rules:
    # Critical: Fast burn rate (threatens 99.95% SLO)
    - alert: YugabyteSLOFastBurn
      expr: yugabyte:error_rate_5m > 0.01  # 1% error rate over 5min
      for: 5m
      labels:
        severity: critical
        service: yugabytedb
        slo: availability
      annotations:
        summary: "YugabyteDB SLO fast burn detected"
        description: "Error rate {{ $value | humanizePercentage }} over 5 minutes threatens 99.95% SLO"
        runbook: "https://runbooks.company.com/yugabyte-slo"
        
    # Warning: Slow burn rate
    - alert: YugabyteSLOSlowBurn  
      expr: yugabyte:error_rate_1h > 0.005  # 0.5% error rate over 1hr
      for: 15m
      labels:
        severity: warning
        service: yugabytedb
        slo: availability
      annotations:
        summary: "YugabyteDB SLO slow burn detected"
        description: "Error rate {{ $value | humanizePercentage }} over 1 hour threatens monthly SLO budget"
        
  - name: yugabyte.cluster.alerts
    rules:
    # Under-replicated tablets (critical data integrity issue)
    - alert: YugabyteUnderReplicatedTablets
      expr: yb_total_under_replicated_tablets{job="yugabyte-master"} > 0
      for: 1m
      labels:
        severity: critical
        service: yugabytedb
        component: tablets
      annotations:
        summary: "YugabyteDB has under-replicated tablets"
        description: "{{ $value }} tablets are under-replicated in {{ $labels.namespace }}"
        impact: "Data availability and durability at risk"
        
    # Master leader missing
    - alert: YugabyteMasterLeaderMissing
      expr: |
        (
          sum by (namespace) (yb_master_is_leader{job="yugabyte-master"}) == 0
        ) and on (namespace) (
          count by (namespace) (up{job="yugabyte-master"}) >= 2
        )
      for: 1m
      labels:
        severity: critical
        service: yugabytedb
        component: master
      annotations:
        summary: "YugabyteDB master leader election failed"
        description: "No master leader found in {{ $labels.namespace }} despite {{ $value }} masters available"
        
    # High p99 latency
    - alert: YugabyteHighLatency
      expr: |
        histogram_quantile(0.99, 
          rate(yb_sql_query_response_time_bucket{job="yugabyte-tserver"}[5m])
        ) > 0.3  # 300ms
      for: 5m
      labels:
        severity: warning
        service: yugabytedb
        component: performance
      annotations:
        summary: "YugabyteDB high p99 latency"
        description: "p99 latency is {{ $value | humanizeDuration }} in {{ $labels.namespace }}"
        
    # Low connection availability
    - alert: YugabyteConnectionsSaturated
      expr: |
        (
          sum by (namespace) (yb_node_connections{job="yugabyte-tserver"}) /
          sum by (namespace) (yb_node_max_connections{job="yugabyte-tserver"})
        ) > 0.8
      for: 5m
      labels:
        severity: warning
        service: yugabyte
        component: connections
      annotations:
        summary: "YugabyteDB connection pool saturation"
        description: "{{ $value | humanizePercentage }} of connections used in {{ $labels.namespace }}"
        
  - name: yugabyte.cdc.alerts  
    rules:
    # CDC lag exceeding SLO
    - alert: YugabyteCDCLagHigh
      expr: yb_cdc_lag_seconds{job="yugabyte-tserver"} > 60
      for: 2m
      labels:
        severity: warning
        service: yugabytedb
        component: cdc
      annotations:
        summary: "YugabyteDB CDC lag exceeds SLO"
        description: "CDC lag is {{ $value | humanizeDuration }} for stream {{ $labels.stream_id }}"
        
    # CDC stream failure
    - alert: YugabyteCDCStreamDown
      expr: yb_cdc_stream_status{job="yugabyte-tserver"} == 0
      for: 1m
      labels:
        severity: critical
        service: yugabytedb
        component: cdc
      annotations:
        summary: "YugabyteDB CDC stream failure"
        description: "CDC stream {{ $labels.stream_id }} is down in {{ $labels.namespace }}"

---
# Redpanda Alert Rules
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: redpanda-alerts
  namespace: monitoring
  labels:
    app: redpanda
    release: kube-prometheus-stack
spec:
  groups:
  - name: redpanda.cluster.alerts
    rules:
    # ISR < 2 (replication at risk)
    - alert: RedpandaISRTooLow
      expr: |
        redpanda_kafka_partitions_in_sync_replicas{job="redpanda"} < 2
      for: 1m
      labels:
        severity: critical
        service: redpanda
        component: replication
      annotations:
        summary: "Redpanda partition has insufficient replicas"
        description: "Topic {{ $labels.topic }} partition {{ $labels.partition }} has only {{ $value }} in-sync replicas"
        
    # Leader imbalance
    - alert: RedpandaLeaderImbalance
      expr: |
        (
          max by (cluster) (redpanda_kafka_leaders_per_broker{job="redpanda"}) -
          min by (cluster) (redpanda_kafka_leaders_per_broker{job="redpanda"})
        ) > 10
      for: 5m
      labels:
        severity: warning
        service: redpanda
        component: leadership
      annotations:
        summary: "Redpanda leader imbalance detected"
        description: "Leader difference of {{ $value }} partitions between brokers"
        
    # Broker down
    - alert: RedpandaBrokerDown
      expr: up{job="redpanda"} == 0
      for: 1m
      labels:
        severity: critical
        service: redpanda
        component: broker
      annotations:
        summary: "Redpanda broker is down"
        description: "Redpanda broker {{ $labels.instance }} in {{ $labels.namespace }} is unreachable"

---
# Debezium Alert Rules  
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: debezium-alerts
  namespace: monitoring
  labels:
    app: debezium
    release: kube-prometheus-stack
spec:
  groups:
  - name: debezium.connector.alerts
    rules:
    # Connector lag exceeding threshold
    - alert: DebeziumLagHigh
      expr: debezium_lag_seconds{job="debezium"} > 60
      for: 2m
      labels:
        severity: warning
        service: debezium
        component: connector
      annotations:
        summary: "Debezium connector lag is high"
        description: "Connector {{ $labels.connector }} lag is {{ $value | humanizeDuration }}"
        
    # Connector status not running
    - alert: DebeziumConnectorDown
      expr: debezium_connector_status{job="debezium"} != 1
      for: 1m
      labels:
        severity: critical
        service: debezium
        component: connector
      annotations:
        summary: "Debezium connector is not running"
        description: "Connector {{ $labels.connector }} status is {{ $labels.status }}" 