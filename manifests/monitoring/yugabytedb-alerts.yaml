# YugabyteDB Multi-Cluster Alert Rules with Enhanced Client Activity Monitoring
# Enhanced with Tenant-Level Governance Alerts

apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: yugabytedb-multi-cluster-alerts
  namespace: monitoring
  labels:
    app: yugabytedb
    prometheus: kube-prometheus
    role: alert-rules
spec:
  groups:
    - name: yugabytedb.cluster.health
      interval: 30s
      rules:
        - alert: YugabyteDBMasterDown
          expr: up{job="yugabyte-master-helm", kubernetes_namespace=~"codet-.*"} == 0
          for: 2m
          labels:
            severity: critical
            cluster: "{{ $labels.kubernetes_namespace }}"
            service: yugabytedb
          annotations:
            summary: "YugabyteDB Master is down"
            description: "YugabyteDB Master {{ $labels.instance }} in cluster {{ $labels.kubernetes_namespace }} has been down for more than 2 minutes."
            runbook_url: "https://docs.yugabyte.com/latest/troubleshoot/"
            
        - alert: YugabyteDBTServerDown
          expr: up{job="yugabyte-tserver-helm", kubernetes_namespace=~"codet-.*"} == 0
          for: 2m
          labels:
            severity: critical
            cluster: "{{ $labels.kubernetes_namespace }}"
            service: yugabytedb
          annotations:
            summary: "YugabyteDB TServer is down"
            description: "YugabyteDB TServer {{ $labels.instance }} in cluster {{ $labels.kubernetes_namespace }} has been down for more than 2 minutes."
            runbook_url: "https://docs.yugabyte.com/latest/troubleshoot/"

        - alert: YugabyteDBClusterUnhealthy
          expr: count by (kubernetes_namespace) (up{job=~"yugabyte-.*", kubernetes_namespace=~"codet-.*"} == 0) > 0
          for: 1m
          labels:
            severity: warning
            cluster: "{{ $labels.kubernetes_namespace }}"
            service: yugabytedb
          annotations:
            summary: "YugabyteDB cluster has unhealthy nodes"
            description: "YugabyteDB cluster {{ $labels.kubernetes_namespace }} has {{ $value }} unhealthy nodes."

    - name: yugabytedb.performance
      interval: 30s
      rules:
        - alert: YugabyteDBHighLatency
          expr: histogram_quantile(0.99, rate(yugabytedb_sql_latency_bucket{kubernetes_namespace=~"codet-.*"}[5m])) > 1000
          for: 5m
          labels:
            severity: warning
            cluster: "{{ $labels.kubernetes_namespace }}"
            service: yugabytedb
          annotations:
            summary: "YugabyteDB high query latency"
            description: "YugabyteDB P99 latency in cluster {{ $labels.kubernetes_namespace }} is {{ $value }}ms, which is above 1000ms threshold."

        - alert: YugabyteDBHighReplicationLag
          expr: yugabytedb_cdc_max_apply_index_lag_ms{kubernetes_namespace=~"codet-.*"} > 5000
          for: 5m
          labels:
            severity: warning
            cluster: "{{ $labels.kubernetes_namespace }}"
            service: yugabytedb
          annotations:
            summary: "YugabyteDB high replication lag"
            description: "YugabyteDB replication lag in cluster {{ $labels.kubernetes_namespace }} is {{ $value }}ms, which indicates potential cross-region replication issues."

        - alert: YugabyteDBHighConnectionCount
          expr: yugabytedb_node_connections{kubernetes_namespace=~"codet-.*"} > 500
          for: 5m
          labels:
            severity: warning
            cluster: "{{ $labels.kubernetes_namespace }}"
            service: yugabytedb
          annotations:
            summary: "YugabyteDB high connection count"
            description: "YugabyteDB instance {{ $labels.instance }} in cluster {{ $labels.kubernetes_namespace }} has {{ $value }} connections, which is above the recommended threshold."

        - alert: YugabyteDBHighMemoryUsage
          expr: (yugabytedb_node_memory_used_bytes{kubernetes_namespace=~"codet-.*"} / yugabytedb_node_memory_total_bytes{kubernetes_namespace=~"codet-.*"}) * 100 > 85
          for: 10m
          labels:
            severity: warning
            cluster: "{{ $labels.kubernetes_namespace }}"
            service: yugabytedb
          annotations:
            summary: "YugabyteDB high memory usage"
            description: "YugabyteDB instance {{ $labels.instance }} in cluster {{ $labels.kubernetes_namespace }} is using {{ $value }}% of available memory."

        - alert: YugabyteDBLowDiskSpace
          expr: (kubelet_volume_stats_used_bytes{namespace=~"codet-.*", persistentvolumeclaim=~".*yb.*"} / kubelet_volume_stats_capacity_bytes{namespace=~"codet-.*", persistentvolumeclaim=~".*yb.*"}) * 100 > 80
          for: 10m
          labels:
            severity: warning
            cluster: "{{ $labels.namespace }}"
            service: yugabytedb
          annotations:
            summary: "YugabyteDB low disk space"
            description: "YugabyteDB persistent volume {{ $labels.persistentvolumeclaim }} in cluster {{ $labels.namespace }} is {{ $value }}% full."

    # Client Activity and Resource Governance Alerts
    - name: yugabytedb.client.activity
      interval: 60s
      rules:
        - alert: YugabyteDBNoisyClient
          expr: |
            (
              sum by(client_ip, kubernetes_namespace) (
                rate(pg_stat_statements_total_time_seconds_total{kubernetes_namespace=~"codet-.*"}[5m])
              ) 
              / 
              sum by(kubernetes_namespace) (
                rate(node_cpu_seconds_total{mode!="idle"}[5m])
              )
            ) * 100 > 40
          for: 5m
          labels:
            severity: warning
            cluster: "{{ $labels.kubernetes_namespace }}"
            service: yugabytedb
            client_ip: "{{ $labels.client_ip }}"
          annotations:
            summary: "YugabyteDB client consuming excessive resources"
            description: "Client {{ $labels.client_ip }} in cluster {{ $labels.kubernetes_namespace }} is consuming {{ $value }}% of cluster CPU resources."
            runbook_url: "https://docs.yugabyte.com/latest/secure/authorization/"

        - alert: YugabyteDBHighClientConnections
          expr: |
            sum by(client_ip, kubernetes_namespace) (
              ysql_connections{kubernetes_namespace=~"codet-.*"}
            ) > 100
          for: 5m
          labels:
            severity: warning
            cluster: "{{ $labels.kubernetes_namespace }}"
            service: yugabytedb
            client_ip: "{{ $labels.client_ip }}"
          annotations:
            summary: "YugabyteDB client has excessive connections"
            description: "Client {{ $labels.client_ip }} in cluster {{ $labels.kubernetes_namespace }} has {{ $value }} active connections."

        - alert: YugabyteDBSlowQueries
          expr: |
            sum by(client_ip, kubernetes_namespace) (
              rate(pg_stat_statements_calls_total{kubernetes_namespace=~"codet-.*"}[5m])
            ) > 1000
          for: 5m
          labels:
            severity: info
            cluster: "{{ $labels.kubernetes_namespace }}"
            service: yugabytedb
            client_ip: "{{ $labels.client_ip }}"
          annotations:
            summary: "YugabyteDB client generating high query volume"
            description: "Client {{ $labels.client_ip }} in cluster {{ $labels.kubernetes_namespace }} is executing {{ $value }} queries per second."

    # NEW: Tenant-Level Governance Alerts
    - name: yugabytedb.tenant.governance
      interval: 60s
      rules:
        - alert: TenantCPUHog
          expr: |
            (
              sum by(tenant_id, kubernetes_namespace) (
                rate(pg_stat_statements_by_tenant_total_time[5m])
              ) 
              / 
              sum by(kubernetes_namespace) (
                rate(node_cpu_seconds_total{mode!="idle"}[5m])
              )
            ) > 0.30
          for: 5m
          labels:
            severity: warning
            cluster: "{{ $labels.kubernetes_namespace }}"
            service: yugabytedb
            tenant_id: "{{ $labels.tenant_id }}"
          annotations:
            summary: "Tenant consuming excessive CPU"
            description: "Tenant {{ $labels.tenant_id }} in cluster {{ $labels.kubernetes_namespace }} is consuming {{ $value | humanizePercentage }} of cluster CPU resources."
            remediation: "Consider throttling tenant queries or migrating to dedicated cluster"

        - alert: TenantMemorySpill
          expr: |
            rate(pg_stat_statements_by_tenant_temp_blks_written[5m]) * 8192 > 1073741824
          for: 5m
          labels:
            severity: warning
            cluster: "{{ $labels.kubernetes_namespace }}"
            service: yugabytedb
            tenant_id: "{{ $labels.tenant_id }}"
          annotations:
            summary: "Tenant causing excessive memory spill"
            description: "Tenant {{ $labels.tenant_id }} in cluster {{ $labels.kubernetes_namespace }} is writing {{ $value | humanizeBytes }} of temp data per second, indicating memory pressure."
            remediation: "Review work_mem settings and query optimization for tenant"

        - alert: TenantDiskQuotaExceeded
          expr: |
            sum by(tenant_id, kubernetes_namespace) (
              yb_tablet_rocksdb_sst_files_size_bytes{table_name=~".*"}
            ) > 10737418240
          for: 5m
          labels:
            severity: critical
            cluster: "{{ $labels.kubernetes_namespace }}"
            service: yugabytedb
            tenant_id: "{{ $labels.tenant_id }}"
          annotations:
            summary: "Tenant exceeded disk quota"
            description: "Tenant {{ $labels.tenant_id }} in cluster {{ $labels.kubernetes_namespace }} is using {{ $value | humanizeBytes }} of disk space, exceeding 10GB limit."
            remediation: "Implement data retention policies or migrate tenant to larger cluster"

        - alert: TenantLongRunningQueries
          expr: |
            pg_long_running_queries_long_queries > 5
          for: 2m
          labels:
            severity: warning
            cluster: "{{ $labels.database }}"
            service: yugabytedb
            tenant_id: "{{ $labels.tenant_id }}"
          annotations:
            summary: "Tenant has multiple long-running queries"
            description: "Tenant {{ $labels.tenant_id }} has {{ $value }} queries running longer than 5 minutes. Max duration: {{ $labels.max_duration_seconds }}s."
            remediation: "Review query performance and consider terminating runaway queries"

        - alert: TenantConnectionLimitApproaching
          expr: |
            pg_database_limits_connection_usage_percent > 80
          for: 5m
          labels:
            severity: warning
            cluster: "{{ $labels.database }}"
            service: yugabytedb
          annotations:
            summary: "Database connection limit approaching"
            description: "Database {{ $labels.database }} is using {{ $value }}% of its connection limit ({{ $labels.connection_limit }})."
            remediation: "Review connection pooling or increase database connection limit"

        - alert: TenantConnectionLimitExceeded
          expr: |
            pg_database_limits_connection_usage_percent >= 100
          for: 1m
          labels:
            severity: critical
            cluster: "{{ $labels.database }}"
            service: yugabytedb
          annotations:
            summary: "Database connection limit exceeded"
            description: "Database {{ $labels.database }} has reached its connection limit ({{ $labels.connection_limit }})."
            remediation: "Immediate action required: kill idle connections or increase limit"

    # Infrastructure and Volume Health Alerts
    - name: yugabytedb.infrastructure
      interval: 60s
      rules:
        - alert: DiskHealthDegraded
          expr: |
            smartctl_device_smart_healthy{kubernetes_namespace=~"codet-.*"} == 0
          for: 0m
          labels:
            severity: critical
            cluster: "{{ $labels.kubernetes_namespace }}"
            service: infrastructure
          annotations:
            summary: "Disk health degraded"
            description: "SMART health check failed for disk {{ $labels.device }} on node {{ $labels.instance }}. Immediate attention required."

        - alert: DiskTemperatureHigh
          expr: |
            smartctl_device_temperature_celsius{kubernetes_namespace=~"codet-.*"} > 60
          for: 10m
          labels:
            severity: warning
            cluster: "{{ $labels.kubernetes_namespace }}"
            service: infrastructure
          annotations:
            summary: "Disk temperature too high"
            description: "Disk {{ $labels.device }} on node {{ $labels.instance }} temperature is {{ $value }}°C, above 60°C threshold."

        - alert: VolumeInodeExhaustion
          expr: |
            (
              kubelet_volume_stats_inodes_used{namespace=~"codet-.*", persistentvolumeclaim=~".*yb.*"} 
              / 
              kubelet_volume_stats_inodes{namespace=~"codet-.*", persistentvolumeclaim=~".*yb.*"}
            ) * 100 > 90
          for: 5m
          labels:
            severity: warning
            cluster: "{{ $labels.namespace }}"
            service: yugabytedb
          annotations:
            summary: "Volume inode exhaustion"
            description: "Volume {{ $labels.persistentvolumeclaim }} in cluster {{ $labels.namespace }} is {{ $value }}% full of inodes."

        - alert: HighIOWait
          expr: |
            rate(node_cpu_seconds_total{mode="iowait"}[5m]) * 100 > 30
          for: 5m
          labels:
            severity: warning
            service: infrastructure
            node: "{{ $labels.instance }}"
          annotations:
            summary: "High IO wait on node"
            description: "Node {{ $labels.instance }} has {{ $value }}% IO wait, indicating storage bottleneck."

    - name: yugabytedb.backup
      interval: 300s
      rules:
        - alert: YugabyteDBBackupFailed
          expr: (time() - yugabytedb_backup_last_successful_time{kubernetes_namespace=~"codet-.*"}) > 86400
          for: 0m
          labels:
            severity: critical
            cluster: "{{ $labels.kubernetes_namespace }}"
            service: yugabytedb
          annotations:
            summary: "YugabyteDB backup failed"
            description: "YugabyteDB backup in cluster {{ $labels.kubernetes_namespace }} has not completed successfully for more than 24 hours."

        - alert: YugabyteDBBackupWarning
          expr: (time() - yugabytedb_backup_last_successful_time{kubernetes_namespace=~"codet-staging-yb|codet-prod-yb"}) > 28800
          for: 0m
          labels:
            severity: warning
            cluster: "{{ $labels.kubernetes_namespace }}"
            service: yugabytedb
          annotations:
            summary: "YugabyteDB backup overdue"
            description: "YugabyteDB backup in cluster {{ $labels.kubernetes_namespace }} has not completed for more than 8 hours."

        - alert: BackupSizeUnexpected
          expr: |
            abs(
              yugabytedb_backup_size_bytes{kubernetes_namespace=~"codet-.*"} 
              - 
              yugabytedb_backup_size_bytes{kubernetes_namespace=~"codet-.*"} offset 1d
            ) / yugabytedb_backup_size_bytes{kubernetes_namespace=~"codet-.*"} offset 1d > 0.5
          for: 0m
          labels:
            severity: warning
            cluster: "{{ $labels.kubernetes_namespace }}"
            service: yugabytedb
          annotations:
            summary: "YugabyteDB backup size changed significantly"
            description: "Backup size for cluster {{ $labels.kubernetes_namespace }} changed by {{ $value }}% compared to yesterday."

    - name: yugabytedb.multi-cluster
      interval: 60s
      rules:
        - alert: YugabyteDBCrossRegionConnectivityLoss
          expr: sum by (source_cluster) (up{job="yugabyte-master-helm", kubernetes_namespace=~"codet-.*"}) < 3
          for: 5m
          labels:
            severity: critical
            service: yugabytedb
          annotations:
            summary: "YugabyteDB cross-region connectivity loss"
            description: "YugabyteDB multi-cluster setup has lost connectivity to one or more regions. Only {{ $value }} clusters are reachable."

        - alert: YugabyteDBReplicationPartition
          expr: abs(count by (kubernetes_namespace) (yugabytedb_cluster_num_tablet_servers{kubernetes_namespace=~"codet-.*"}) - 1) > 0
          for: 10m
          labels:
            severity: warning
            cluster: "{{ $labels.kubernetes_namespace }}"
            service: yugabytedb
          annotations:
            summary: "YugabyteDB replication partition detected"
            description: "YugabyteDB cluster {{ $labels.kubernetes_namespace }} may be experiencing replication partitioning."

        - alert: TabletLeaderImbalance
          expr: |
            stddev by (kubernetes_namespace) (
              count by (instance, kubernetes_namespace) (
                tablet_raft_is_leader{kubernetes_namespace=~"codet-.*"} == 1
              )
            ) > 5
          for: 15m
          labels:
            severity: warning
            cluster: "{{ $labels.kubernetes_namespace }}"
            service: yugabytedb
          annotations:
            summary: "YugabyteDB tablet leader imbalance"
            description: "Cluster {{ $labels.kubernetes_namespace }} has uneven tablet leader distribution across nodes."

    - name: kubernetes.yugabytedb
      interval: 60s
      rules:
        - alert: YugabyteDBPodCrashLooping
          expr: rate(kube_pod_container_status_restarts_total{namespace=~"codet-.*", pod=~"yb-.*"}[15m]) > 0
          for: 5m
          labels:
            severity: warning
            cluster: "{{ $labels.namespace }}"
            service: yugabytedb
          annotations:
            summary: "YugabyteDB pod crash looping"
            description: "YugabyteDB pod {{ $labels.pod }} in cluster {{ $labels.namespace }} is crash looping."

        - alert: YugabyteDBPodNotReady
          expr: kube_pod_status_ready{namespace=~"codet-.*", pod=~"yb-.*", condition="false"} == 1
          for: 10m
          labels:
            severity: warning
            cluster: "{{ $labels.namespace }}"
            service: yugabytedb
          annotations:
            summary: "YugabyteDB pod not ready"
            description: "YugabyteDB pod {{ $labels.pod }} in cluster {{ $labels.namespace }} has been not ready for more than 10 minutes."

        - alert: YugabyteDBPVCSpaceRunningOut
          expr: predict_linear(kubelet_volume_stats_used_bytes{namespace=~"codet-.*", persistentvolumeclaim=~".*yb.*"}[6h], 24*3600) / kubelet_volume_stats_capacity_bytes{namespace=~"codet-.*", persistentvolumeclaim=~".*yb.*"} > 0.9
          for: 5m
          labels:
            severity: warning
            cluster: "{{ $labels.namespace }}"
            service: yugabytedb
          annotations:
            summary: "YugabyteDB PVC space running out"
            description: "YugabyteDB PVC {{ $labels.persistentvolumeclaim }} in cluster {{ $labels.namespace }} is predicted to run out of space within 24 hours."

        - alert: YugabyteDBResourceQuotaExceeded
          expr: |
            (
              kube_resourcequota_used{namespace=~"codet-.*", resource="requests.cpu"} 
              / 
              kube_resourcequota_hard{namespace=~"codet-.*", resource="requests.cpu"}
            ) > 0.9
          for: 5m
          labels:
            severity: warning
            cluster: "{{ $labels.namespace }}"
            service: kubernetes
          annotations:
            summary: "Resource quota nearly exceeded"
            description: "Namespace {{ $labels.namespace }} is using {{ $value }}% of CPU resource quota."

---
# Enhanced AlertManager Configuration with Tenant-Specific Routing
apiVersion: v1
kind: Secret
metadata:
  name: alertmanager-config
  namespace: monitoring
  labels:
    app: alertmanager
type: Opaque
stringData:
  alertmanager.yml: |
    global:
      smtp_smarthost: 'smtp-relay.kube-system.svc.cluster.local:587'
      smtp_from: 'alertmanager@codet.com'
      smtp_require_tls: true
    
    route:
      group_by: ['alertname', 'cluster', 'service', 'severity', 'tenant_id']
      group_wait: 10s
      group_interval: 10s
      repeat_interval: 1h
      receiver: 'default-receiver'
      routes:
      # Critical alerts for production - immediate response
      - match:
          severity: critical
        match_re:
          cluster: "codet-prod-yb"
        receiver: 'pager-critical'
        group_wait: 0s
        repeat_interval: 5m
      
      # Tenant governance alerts - dedicated team
      - match_re:
          alertname: "Tenant.*|TenantCPUHog|TenantMemorySpill|TenantDiskQuotaExceeded"
        receiver: 'tenant-governance-team'
        repeat_interval: 30m
      
      # Client activity alerts - governance team
      - match_re:
          alertname: "YugabyteDBNoisyClient|YugabyteDBHighClientConnections"
        receiver: 'governance-team'
        repeat_interval: 30m
      
      # Infrastructure alerts - ops team
      - match_re:
          alertname: "DiskHealth.*|HighIOWait|VolumeInodeExhaustion"
        receiver: 'infrastructure-team'
        repeat_interval: 2h
      
      # General warnings
      - match:
          severity: warning
        receiver: 'slack-warnings'
        repeat_interval: 12h
    
    receivers:
    - name: 'default-receiver'
      webhook_configs:
      - url: 'http://webhook-service.monitoring.svc.cluster.local:5001/webhook'
        send_resolved: true
    
    - name: 'pager-critical'
      # Configure with your PagerDuty integration key
      webhook_configs:
      - url: 'https://events.pagerduty.com/v2/enqueue'
        send_resolved: true
        http_config:
          bearer_token_file: /etc/alertmanager/secrets/pagerduty-key
    
    - name: 'tenant-governance-team'
      email_configs:
      - to: 'tenant-governance@codet.com'
        subject: 'Tenant Governance Alert - {{ .GroupLabels.alertname }}'
        body: |
          Alert: {{ .GroupLabels.alertname }}
          Cluster: {{ .GroupLabels.cluster }}
          Tenant: {{ .GroupLabels.tenant_id }}
          
          {{ range .Alerts }}
          Summary: {{ .Annotations.summary }}
          Description: {{ .Annotations.description }}
          Remediation: {{ .Annotations.remediation }}
          {{ end }}
          
          Immediate Actions Available:
          1. Reduce tenant connection limits
          2. Throttle tenant queries via work_mem
          3. Move tenant to dedicated cluster
          4. Kill runaway queries
      slack_configs:
      - api_url_file: /etc/alertmanager/secrets/slack-webhook-url
        channel: '#tenant-governance'
        title: 'Tenant Alert - {{ .GroupLabels.tenant_id }}'
        text: |
          *Tenant:* {{ .GroupLabels.tenant_id }}
          *Cluster:* {{ .GroupLabels.cluster }}
          *Alert:* {{ .GroupLabels.alertname }}
          
          {{ range .Alerts }}
          *Issue:* {{ .Annotations.summary }}
          *Remediation:* {{ .Annotations.remediation }}
          {{ end }}
        send_resolved: true
    
    - name: 'governance-team'
      email_configs:
      - to: 'governance@codet.com'
        subject: 'Client Resource Governance Alert - {{ .GroupLabels.alertname }}'
        body: |
          Alert: {{ .GroupLabels.alertname }}
          Cluster: {{ .GroupLabels.cluster }}
          Client: {{ .GroupLabels.client_ip }}
          
          {{ range .Alerts }}
          Summary: {{ .Annotations.summary }}
          Description: {{ .Annotations.description }}
          {{ end }}
          
          Recommended Actions:
          1. Check client {{ .GroupLabels.client_ip }} query patterns
          2. Consider connection limits or resource quotas
          3. Review with client team for optimization
    
    - name: 'infrastructure-team'
      email_configs:
      - to: 'infrastructure@codet.com'
        subject: 'Infrastructure Alert - {{ .GroupLabels.alertname }}'
        body: |
          Alert: {{ .GroupLabels.alertname }}
          Node/Cluster: {{ .GroupLabels.instance }}{{ .GroupLabels.cluster }}
          
          {{ range .Alerts }}
          Description: {{ .Annotations.description }}
          {{ end }}
    
    - name: 'slack-warnings'
      slack_configs:
      - api_url_file: /etc/alertmanager/secrets/slack-webhook-url
        channel: '#yugabytedb-alerts'
        title: 'YugabyteDB Alert - {{ .GroupLabels.alertname }}'
        text: |
          *Cluster:* {{ .GroupLabels.cluster }}
          *Severity:* {{ .GroupLabels.severity }}
          {{ if .GroupLabels.tenant_id }}*Tenant:* {{ .GroupLabels.tenant_id }}{{ end }}
          
          {{ range .Alerts }}
          *Summary:* {{ .Annotations.summary }}
          *Details:* {{ .Annotations.description }}
          {{ end }}
        send_resolved: true 